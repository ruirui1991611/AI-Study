回归算法
	1)线性回归：
		a. 概念(个人理解)：特征属性x与目标属性y之间的映射关系是线性的，针对这些数据构建的模型称为线性回归
		b. 一般形式(不考虑损失函数)：hθ(x) = θ0*1 + θ1*x1 + θ2*x2 + ... + θn*xn = θT*x		(θT为θ矩阵的转秩，θ和x通常看成列向量) ==> 根据已有数据，计算并选择最优的θ值
		c. 损失(loss)函数：y = θT*x + ε (y，x，θ，ε均为列向量，ε为误差，服从均值为0，方差为某定值δ^2的高斯分布) ==> ε = y - θT*x，取ln对数求得 ==> J(θ) = 1/2 * Σ(θT*x - y)^2(其中累加有1到m个样本，x有n维特征，均为已知)
		d. 求解方式：
			① -- 直接对损失函数(c中的J(θ))求最小值(普通最小二乘法得到θ的参数解析式，推导过程这里略去) ==> θ = (XT*X)^-1 * XT*Y	(注：X,Y为已知数据的矩阵，XT为矩阵的转秩)
			② -- 梯度下降(Gradient Descent)
				【1】批量梯度下降(BGD)：每次更新参数时都使用所有的样本来进行更新
					- 优点：全局最优解；易于并行实现
					- 缺点：当样本数目很多时，训练过程会很慢
				【2】随机梯度下降(SGD)：每次更新参数时都随机使用一个样本来进行更新
					- 优点：训练速度快
					- 缺点：会出现局部最优解，整体呈波动曲线；
				【3】小批量梯度下降(MBGD)：更新每一参数时都使用一部分样本来进行更新
				【4】综述：如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法
	
		e. 过拟合问题(模型学习了太多训练集的数据特征，导致模型在训练集上效果非常不错，但是在测试集上效果比较差)
			解决方案(一般选择②)：
				① -- 采用L1正则,即再c中得到的J(θ)加上一个1范式 ==> J(θ) = 1/2 * Σ(θT*x - y)^2 + λ*Σ(|θ|) (其中第一项累加有1到m个样本，x有n维特征，第二项累加即为1范式，有1到n个θ值，与x保持一致)
					- 结果：会导致训练模型的参数出现为0的情况，即稀疏解，常用于特征选择的过程中
				② -- 采用L2正则,即再c中得到的J(θ)加上一个2范式 ==> J(θ) = 1/2 * Σ(θT*x - y)^2 + λ*Σ(θ^2) (其中第一项累加有1到m个样本，x有n维特征，第二项累加即为2范式，有1到n个θ值，与x保持一致)
					- 结果：一般会得到比较接近实际情况的较优解
	
	2)Logistci回归：
	
	3)Softmax回归：
	
	