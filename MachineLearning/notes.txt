一. 回归算法：
	1)线性回归：
		a. 概念(个人理解)：特征属性x与目标属性y之间的映射关系是线性的，针对这些数据构建的模型称为线性回归
		b. 一般形式(不考虑损失函数)：hθ(x) = θ0*1 + θ1*x1 + θ2*x2 + ... + θn*xn = θT*x		(θT为θ矩阵的转秩，θ和x通常看成列向量) ==> 根据已有数据，计算并选择最优的θ值
		c. 损失(loss)函数：y = θT*x + ε (y，x，θ，ε均为列向量，ε为误差，服从均值为0，方差为某定值δ^2的高斯分布) ==> ε = y - θT*x，取ln对数求得 ==> J(θ) = 1/2 * Σ(θT*x - y)^2(其中累加有1到m个样本，x有n维特征，均为已知)
		d. 求解参数θ方式：
			① -- 直接对损失函数(c中的J(θ))求最小值(普通最小二乘法得到θ的参数解析式，推导过程这里略去) ==> θ = (XT*X)^-1 * XT*Y	(注：X,Y为已知数据的矩阵，XT为矩阵的转秩)
			② -- 梯度下降(Gradient Descent)
				【1】批量梯度下降(BGD)：每次更新参数时都使用所有的样本来进行更新
					- 优点：全局最优解；易于并行实现
					- 缺点：当样本数目很多时，训练过程会很慢
				【2】随机梯度下降(SGD)：每次更新参数时都随机使用一个样本来进行更新
					- 优点：训练速度快，适合样本数据量大的情况以及在线机器学习(Online Machine Learning）
					- 缺点：会出现局部最优解，整体呈波动曲线；
				【3】小批量梯度下降(MBGD)：更新每一参数时都使用一部分样本来进行更新
				【4】综述：如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法
	
		e. 过拟合问题(模型学习了太多训练集的数据特征，导致模型在训练集上效果非常不错，但是在测试集上效果比较差)
			解决方案(一般选择②，下面的λ和p参数称之为超参，使用时需要人工来调整，通常使用交叉验证的方式，来找到最优参数)：
				①(Lasso算法) -- 采用L1正则,即再c中得到的J(θ)加上一个1范式 ==> J(θ) = 1/2 * Σ(θT*x - y)^2 + λ*Σ(|θ|) (其中第一项累加有1到m个样本，x有n维特征，第二项累加即为1范式，有1到n个θ值，与x保持一致)
					- 结果：会导致训练模型的参数出现为0的情况，即稀疏解，常用于特征选择的过程中
				②(Ridge算法) -- 采用L2正则,即再c中得到的J(θ)加上一个2范式 ==> J(θ) = 1/2 * Σ(θT*x - y)^2 + λ*Σ(θ^2) (其中第一项累加有1到m个样本，x有n维特征，第二项累加即为2范式，有1到n个θ值，与x保持一致)
					- 结果：一般会得到比较接近实际情况的较优解
				③(Elasitc Net 弹性网络) -- 将①和②结合，加入参数p，作为L1正则和L2正则的使用比例 ==> J(θ) = 1/2 * Σ(θT*x - y)^2 + λ*((p)Σ(|θ|) + (1-p)Σ(θ^2))	
	2)Logistci回归(回归类算法中使用的较多，因为其值域为0到1，可看成概率问题，且函数值在正负无穷大时趋于平缓，比较符合实际的某些应用场景)：
		a. 概念(个人理解)：适用于二分类(A，B)，使用sigmoid函数，将线性回归 hθ(x) = θT*x 的函数值转化为0到1之间的值(看作成概率)，当对于某个样本x的输入，转化后的值>=0.5时，认为其属于A类，否则为B类
		b. 一般形式：由sigmoid函数：g(z) = 1/(1+e^(-z)) ==> hθ(x) = g(θT*x) = 1/(1+e^(θT*x)) 即为逻辑回归的一般形式
		c. 对数似然函数(推导过程这里略去)：l(θ) = Σ(y*log(hθ(x))+(1-y)log(1-(hθ(x))))
		d. 求解参数θ方式(最大化似然估计对应的θ值即为最优θ)：
			① -- 梯度下降(常用方法)	
	3)Softmax回归：
		a. 概念(个人理解)：softmax回归是logistic回归的一般化，对于给定的样本输入x，计算出属于所有类别的概率值(相加为1)，哪个值大，就将其分为哪个类，适用于多(K)分类的问题，对于每一类，找到其对应的参数向量θ(k)

二. KNN(K近邻)算法：
	1) 原理或思想：在给定的样本空间上，距离(通常为欧式距离)较近的样本之间有较高的相似度，具有相同的特征信息(既可用于回归，也可以用于分类)
	2) 算法过程：
		① -- 给定训练集和待测样本，在训练集上找到与待测样本最相似(距离最近)的K个样本
		② -- 根据找到的K个样本数据对应的y(输出)值来预测当前待预测样本的y(输出)值
	3) 算法实现(找出最近邻的K个点)：
		① -- 暴力实现：计算预测样本到所有训练集样本的距离，然后选择最小的k个距离即可得到K个最邻近点
		② -- 构建KD_Tree：对训练数据进行建模，构建KD树，根据建好的模型来找到最近邻点(查找方式如下)
			a. 先在KD树里面找到包含目标点的叶子节点，以目标点为圆心，目标点到叶子节点样本实例最近的距离为半径，得到一个超球体，最近邻的点一定在这个超球体内部
			b. 返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交，就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻
			c. 如果不相交，直接返回父节点的父节点，在另一个子树继续搜索最近邻
			d. 回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻
	4) 预测策略：
		① -- 分类：
			a. 多数表决法(默认)：在K个样本中，出现次数最多的那个类别作为待预测样本的预测类别(看成权重都为1)
			b. 加权多数表决法：在K个样本中，每个样本到待预测样本的距离的倒数作为当前样本的权重系数，统计各个类别对应权重的累加和，最终将权重系数最大的那个对应类别作为待预测样本的类别
		② -- 回归：
			a. 平均值法(默认)：将K个样本的y值的均值作为待预测样本的预测值
			b. 加权平均值法：在K个样本中，每个样本到待预测样本的距离的倒数作为当前样本的权重系数，然后将权重系数归一化并将归一化的权重和y值一起进行加权求均值，即为最终的预测值
			
三. 决策树算法：
	0) 数据环境假设：
		① -- 设训练数据集为D，|D|表示其样本容量，即样本个数。设Y值有K个类Ck，k = 1,2 ... K，|Ck|是属于类Ck的样本个数，则：Σ|Ck| = |D|
		② -- 设特征A有n个不同的取值{a1，a2，... ，an}，根据特征A的取值将D划分为n个子集D1，D2，... ，Dn，|Di|为Di的样本个数，则：Σ|Di| = |D|
		③ -- 记子集Di中属于类Ck的样本的集合为Dik，即有：Dik = Di∩Ck，|Dik|为子集Dik的样本个数
	1) 决策树构建过程：	
		① -- 决策树特征选择
			a. 依据信息增益来选：g(D,A) = H(D) - H(D|A)	    		<-->    (ID3算法使用，值越大越好，即H(D|A)的值越小越好)
				H(D) = -Σ(|Ck|/|D| * log(|Ck|/|D|))	
				H(D|A) = -Σ((|Di|/|D|) * Σ((|Dik|/|Di|)*log(|Dik|/|Di|)))
			b. 依据信息增益率来选：gR(D,A) = g(D,A) / H(D)    		<-->	(C4.5算法使用，值越大越好，即H(D|A)的值越小越好)
			c. 依据Gini系数来选：gGini(D,A) = Gini(D) - Gini(D|A)   <-->	(CART算法使用，值越大越好，即Gini(D|A)的值越小越好 )
				Gini(D) = 1 - Σ((|Ck|/|D|)^2)
				Gini(D|A) = Σ((|Di|/|D|) * (1-Σ((|Dik|/|Di|)^2)))
		② -- 决策树的生成
			a. 遍历所有的特征的划分方式，选择出一个最优的划分方式，其中这个最优划分就是让划分之后的数据集"更纯"
			b. 基于选择出来的划分方式，将数据集分成多个子数据集
			c. 对每个子数据集继续采用相同的操作进行数据的划分(选最优划分方式+划分数据)，直到每个子数据集中只有一个类别的样本或者树的深度达到给定的限制条件的时候，结束划分操作
			d. 对所有的子数据集一致迭代，直到达到限制条件，决策树构建完成
		③ -- 决策树的优化
			a. 欠拟合：
				【1】通过增加树的深度来解决
				【2】利用集成学习中的Adaboost算法来解决(参考集成学习算法部分)
			b. 过拟合:
				【1】限制树的深度(限制树的复杂程度) ---> 剪枝(后置剪枝)
				【2】利用集成学习中的随机森林算法来解决(参考集成学习算法部分)
	2) 利用构建好的决策树进行预测：
		① -- 分类：判断样本x属于哪个叶子节点，以该叶子节点中所有样本中的出现次数最多的那个类别作为样本x的预测类别(类似于KNN中的多数表决法)
		② -- 回归：判断样本x属于哪个叶子节点，以该叶子节点中所有样本的y值的均值作为样本x的预测值(类似于KNN中的平均值法)
	3) 实际应用中的常用算法：
		① -- ID3：分类决策树、只能处理离散的特征数据、构建的是一个多叉树
		② -- C4.5：分类决策树、可以处理离散特征数据和连续特征数据、构建的是一个多叉树
		③ -- CART(sklearn中有API支持，常用)：分类回归决策树、可以处理离散和连续的特征数据、构建的是一个二叉树	
		
			